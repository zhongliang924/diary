# Triton 推理服务入门

## 推理、部署和服务化

### 推理

两层含义：

相对于“训练”（Training）而言的推理，即模型前向计算，对于给定输入计算模型的输出结果；相对“预测”（Prediction）而言的推理，是统计学的范畴。

本文所述推理是第一层含义

### 部署

训练得到的模型的主要目的是为了更有效解决实际问题，因此部署是一个非常重要的阶段！如果训练了一个非常完美的模型，但是这个模型在某些环境中却无法使用（缺乏对应算子实现、模型太大、算力不足等）、又或者运行得非常慢，这在实际生产过程中是无法接受的。模型部署的课题包括但不限于：移植、压缩、加速等。

### 服务化

模型部署的方式是多样的：封装成一个 SDK，集成到 APP 或者服务中；封装成一个 Web 服务，对外暴露接口（HTTP(s)、RPC等协议），本文主要讨论后者。

模型服务化的核心功能包括：服务框架+前向推理计算。前者在业界有许多优秀的框架：如 Google 的 GRPC、百度的 BRPC 等，甚至可以用 Python 的 Flask 和 Tornado 框架；后者需要调用模型框架提供的前向推理 API 来实现，比如 TensorFlow 支持 Python、C++、Java、Go 等多种语言，即使框架支持的语言有限，也可以通过每种语言的提供跨语言调用机制实现模型框架的前向推理 API。

## 推理系统架构设计

对于一个推理系统来说，主要由两个部分组成：客户端和服务端。

对于客户端来说，通常比较简单。它可以是手机、平板、电脑或者下游应用系统等

对于服务端，通常比较复杂，它是整个推理系统的核心：

- 首先，它得有一个**部署平台或集群**（如 K8s），用于一个推理服务的生命周期管理（模型的加载/卸载，模型服务实例的弹性扩容等）
- 然后，它得有一个**负载均衡器**（如 Ingress），解决模型推理实例负载均衡的问题。将客户端的请求，均衡分配给推理服务实例
- 其次，它还应该有一个**模型仓库**（如本地文件系统），对模型权重文件和模型输入输出配置进行管理
- 之后，它还应该有一个**模型监控服务**，用于观察宏观/微观的数据
- 最后，也是最核心的，应该提供**推理服务**，进行模型推理

Triton 就是一个推理服务化框架，这也是 Triton 的定位。

## NVIDIA Triton 推理服务器

NVIDIA Triton™ 推理服务器，作为 NVIDIA AI 平台的一部分，也随 NVIDIA AI Enterprise 提供，是一款开源软件，它标准化了在各种工作负载中部署和执行 AI 模型的流程。

主要特性包括：

1. **多框架支持**：Triton 支持多种深度学习框架，包括 `TensorFlow, Pytorch, TensorRT, ONNX RT, custom backends` 等，这使得用户可以使用它们喜欢的框架来训练和部署模型。
2. **高性能推理**：Triton 利用 NVIDIA 提供的 TensorRT 推理加速库，以及 GPU 硬件强大性能，实现高效的深度学习推理。它适用于对实时性较高的应用，如自动驾驶。
3. **灵活的部署选项**：Triton 支持各种部署选项，包括单机部署、多机部署以及容器化部署。用户可以根据自己的需求选择最合适的应用程序部署方式。
4. **多模型支持**：Triton 允许同时部署多个深度学习模型，并为每个模型提供独立的端点。
5. **RESTful API**：Triton 提供了一个 RESTful API，使得客户端应用程序可以轻松与服务器进行通信，提交推理请求并接收推理结果。
6. **监控和管理**：Triton 提供监控和管理功能，可以让用户跟踪服务器的性能、资源使用情况和模型的状态，并进行必要的管理操作。
7. **开源和社区支持**

Triton 推理服务大体流程为：推理请求通过 HTTP/REST、GRPC 或 C API 到达推理服务，然后路由到每个模型相关的调度队列中；每个模型的调度器可以选择执行推理请求的批处理，然后将请求传递给模型类型对应的后端（backends）；后端使用批处理请求提供的输入执行推理以生成请求的输出；最后返回输出。

Triton 服务架构允许多个模型或同一模型的多个实例在同一系统上并行执行，系统可能有 0 个，1 个或多个 GPU 卡。



