# DeepSpeech 2：英语和普通话端到端语音识别

论文链接：https://arxiv.org/abs/1512.02595

我们展示了一种端到端深度学习方法，可用于识别英语或普通话两种截然不同的语言。由于它用神经网络替代了整个手工设计的组件流水线，端到端学习使我们能够处理各种各样的语音，包括嘈杂的环境、口音和不同的语言。我们方法的关键在于应用高性能计算（HPC）技术，使速度比我们先前的系统提高了 $7$ 倍[26]。由于这种效率，以前需要几周的实验现在能在几天内完成。这使我们能够更快地迭代，以找到更优越的架构和算法。因此，在几种情况下，我们的系统在标准数据集上的基准测试中与人工工作者的转录相竞争。最后，通过在数据中心使用批处理调度和GPU的技术，我们展示了我们的系统可以以低延迟的方式以低成本部署在在线环境中，为大规模用户提供服务。

## 引言

几十年来，大量手工设计的领域知识已经被用于当前最先进的自动语音识别（ASR）流水线。一种简单但强大的替代解决方案**是通过深度学习训练这样的ASR模型，将大多数模块替换为单一模型** [26]。我们呈现了我们语音系统的第二代，展示了端到端学习的主要优势。Deep Speech 2 ASR流水线在多个基准测试中接近或超过了亚马逊机械土耳其人工工作者的准确度，在少量修改的情况下可以处理多种语言，并可在生产环境中部署。因此，它代表着朝着一个单一ASR系统迈出的重要一步，该系统能够处理人类处理的整个语音识别背景范围。由于我们的系统建立在端到端深度学习的基础上，我们可以采用一系列深度学习技术：捕捉大规模训练集，使用高性能计算训练更大的模型，并系统地探索神经网络架构的空间。我们展示了通过这些技术，我们能够将我们先前的端到端系统 [26] 在英语中的错误率降低高达 43%，同时也能够高准确度地识别普通话语音。

语音识别面临的挑战之一是语音和声学的广泛差异。因此，现代ASR流水线由许多组件组成，包括复杂的特征提取、声学模型、语言和发音模型、说话者适应等。构建和调整这些单独的组件使得开发新的语音识别器非常困难，特别是对于一种新语言而言。实际上，**许多部分在不同环境或语言中泛化效果不佳，通常需要支持多个特定于应用的系统，以提供可接受的准确性**。这种现状与人类语音识别不同：人类在童年时期具有学习任何语言的固有能力，使用通用技能学习语言。在学会阅读和写作后，大多数人可以在环境、说话者口音和噪音变异的情况下，无需为转录任务进行额外训练即可转录语音。为了满足语音识别用户的期望，我们认为一个单一引擎必须学会类似地胜任；能够在只进行轻微修改的情况下处理大多数应用，并能够在没有剧烈变化的情况下学习新语言。我们的端到端系统使这一目标成为可能，使我们能够在两种截然不同的语言（普通话和英语）的多个测试中接近或超过人工工作者的表现水平。

由于Deep Speech 2（DS2）是一个端到端深度学习系统，我们可以通过关注三个关键组件来实现性能提升：模型架构、大规模标记训练数据集和计算规模。这种方法在计算机视觉和自然语言等其他应用领域也取得了巨大的进展。**本文详细介绍了我们在这三个方面对语音识别的贡献，包括对模型架构以及数据和模型规模对识别性能的影响的广泛调查**。具体而言，我们描述了使用连接主义时序分类（CTC）损失函数[22]进行训练的神经网络的大量实验，以从音频中预测语音转录。我们考虑由许多层的循环连接、卷积滤波器和非线性组成的网络，以及批量归一化（Batch Normalization，BatchNorm）[63]应用于RNN的特定实例的影响。我们不仅找到了比先前工作[26]中更好的预测的网络，还发现了可以在生产环境中部署的递归模型的实例，而准确性没有显著下降。

除了寻找更好的模型架构外，**深度学习系统极大地受益于大量的训练数据**。我们详细介绍了我们的数据捕捉流水线，使我们能够创建比通常用于训练语音识别系统的数据更大的数据集。我们的英语语音系统训练了11,940小时的语音，而普通话系统训练了9,400小时。我们在训练过程中使用数据合成来进一步增加数据。

在大规模数据上进行训练通常需要使用更大的模型。事实上，我们的模型的参数比我们先前系统中使用的参数要多得多。在这些规模上训练单个模型需要数十exaFLOPs的计算能力，这在单个GPU上执行需要3-6周的时间。这使得模型探索变得非常耗时，因此**我们构建了一个高度优化的训练系统，使用8或16个GPU来训练一个模型**。与之前使用参数服务器和异步更新[18, 10]的大规模训练方法相比，我们使用同步随机梯度下降（SGD），这在测试新想法时更容易调试，而且对于相同程度的数据并行性，收敛速度更快。为了使整个系统高效，我们描述了单个GPU的优化以及多个GPU的可扩展性改进。我们采用在高性能计算中通常找到的优化技术来提高可扩展性。这些优化包括在GPU上对CTC损失函数的快速实现和自定义内存分配器。我们还使用精心集成的计算节点和自定义实现的全局归约来加速跨GPU的通信。总体而言，该系统在使用16个GPU进行训练时维持了约50 teraFLOP/秒的性能。这相当于每个GPU约3 teraFLOP/秒，约为峰值理论性能的50%。这种可扩展性和效率将训练时间缩短到3到5天，使我们能够更快地迭代我们的模型和数据集。

**我们在几个公开可用的测试集上对我们的系统进行基准测试**，并将结果与我们先前的端到端系统[26]进行比较。我们的目标是最终在不仅仅是特定基准测试上达到人类水平的性能，而是在反映各种情境的一系列基准测试上达到人类水平的性能。为此，我们还测量了每个基准测试中人工工作者的表现以进行比较。我们发现我们的系统在一些常见的研究基准测试中表现优于人类，并在更难的情况下显著缩小了差距。除了公共基准测试外，我们还展示了我们的普通话系统在反映真实产品场景的内部数据集上的性能。

在大规模部署时，深度学习系统可能面临一些挑战。对于每个用户的话语，评估大型神经网络在计算上是昂贵的，而某些网络架构比其他网络更容易部署。通过模型探索，我们找到了高准确度、可部署的网络架构，这里进行详细介绍。**我们还采用一种适用于GPU硬件的批处理方案，称为Batch Dispatch，可在生产服务器上实现我们的普通话引擎的高效实时运行**。我们的实现在服务器加载了10个同时音频流的情况下，实现了98th百分位的计算延迟为67毫秒。

论文的剩余部分如下。我们首先在第2节回顾深度学习、端到端语音识别和可扩展性的相关工作。第3节描述了模型的架构和算法改进，第4节解释了如何高效计算它们。我们在第5节讨论了训练数据以及进一步增强训练集所采取的步骤。第6节介绍了DS2系统在英语和普通话中的结果分析。最后，在第7节中，我们描述了将DS2部署到实际用户所需的步骤。

## 相关工作

这项工作受到了深度学习和语音识别领域先前工作的启发。在20多年前，人们就开始探索前馈神经网络声学模型[7, 50, 19]。在大致同一时期，循环神经网络和具有卷积的网络也被用于语音识别[51, 67]。最近，深度神经网络已经成为ASR流水线中的固定组成部分，几乎所有最先进的语音工作都包含某种形式的深度神经网络[42, 29, 17, 16, 43, 58]。卷积网络也被发现对声学模型有益[1, 53]。循环神经网络，通常是LSTM，刚刚开始在最先进的识别器中得到应用[24, 25, 55]，并且与卷积层一起用于特征提取[52]效果良好。同时探索了具有双向[24]和单向循环的模型。

端到端语音识别是一个活跃的研究领域，在重新评分DNN-HMM的输出[23]以及独立使用时显示出引人注目的结果[26]。目前有两种方法用于将可变长度的音频序列直接映射到可变长度的转录。**RNN编码器-解码器范式使用编码器RNN将输入映射到一个固定长度的向量，然后使用解码器网络将这个固定长度的向量扩展为一系列输出预测[11, 62]**。在解码器中添加注意力机制极大地提高了系统的性能，特别是在处理长输入或输出时[2]。在语音领域，具有注意力的RNN编码器-解码器在预测音素[12]或字母[3, 8]方面表现良好。

**另一种常用的将可变长度音频输入映射到可变长度输出的技术是与RNN结合使用的CTC损失函数[22]，用于建模时间信息**。CTC-RNN模型在具有字母输出的端到端语音识别中表现良好[23, 27, 26, 40]。CTC-RNN模型在预测音素方面也表现良好[41, 54]，尽管在这种情况下仍然需要一个词汇表。此外，在这种情况下，还需要使用GMM-HMM系统的逐帧对齐来预训练CTC-RNN网络[54]。相反，我们从头开始训练CTC-RNN网络，无需使用逐帧对齐进行预训练。

**深度学习领域迄今取得的成功在很大程度上归功于对规模的充分利用**[36, 38]。在单个GPU上进行训练导致了显著的性能提升[49]，随后这种提升被线性地扩展到了两个[36]或更多个GPU[15]。我们利用了先前在提高单个GPU效率方面的研究成果[9]。我们基于过去关于使用**模型并行ism[15]、数据并行ism**[18]或两者结合的研究[64, 26]，构建了一个快速且高度可扩展的系统，用于在语音识别中训练深度循环神经网络（deep RNNs）。

数据同样是端到端语音识别成功的关键，Deep Speech 1（DS1）使用了超过7000小时的标记语音数据[26]。**数据增强在提高深度学习在计算机视觉中性能方面表现得非常出色**[39, 56, 14]。这也已经被证明对语音系统的性能提升具有积极作用[21, 26]。在语音数据增强的技术中，从简单的噪声添加[26]到复杂的扰动，如模拟改变说话者的声道长度和语速[31, 35]，涵盖了各种方法。

现有的语音系统还可用于启动新的数据收集。在一种方法中，作者使用一个语音引擎来对齐和过滤一千小时的朗读语音[46]。在另一种方法中，使用一个重量级的离线语音识别器为数万小时的语音生成转录[33]。然后，通过一个过滤器并用于重新训练识别器，从而获得显著的性能提升。我们从这些先前的方法中汲取灵感，**利用启动更大数据集和数据增强的方式，以增加系统中标记数据的有效数量**。

## 模型架构

一个简单的多层模型，只包含单个循环层，无法充分利用数千小时的标记语音。**为了从如此大规模的数据集中学习，我们通过增加深度来提高模型容量**。我们尝试了包括多个双向循环层和卷积层的结构，最多达到11层。这些模型每个数据示例的计算量几乎是Deep Speech 1中的模型的8倍，因此快速优化和计算是至关重要的。为了成功优化这些模型，我们对RNNs使用了批归一化（Batch Normalization）和一个我们称之为SortaGrad的新颖优化课程。我们还利用RNN输入之间的长跨度，将每个示例的计算量降低了3倍。这对于训练和评估都是有益的，尽管需要一些修改以便与CTC（Connectionist Temporal Classification）协同工作。最后，尽管我们的许多研究结果使用了双向循环层，我们发现即使只使用单向循环层，也能得到出色的模型——这一特性使得这些模型更容易部署。总的来说，这些特性使我们能够有效地优化深度循环神经网络，并在英语和普通话的错误率上相较较小的基准模型提高了40%以上的性能。

### 前言

图 **1** 展示了DS2系统的架构，其核心与之前的DS1系统[26]相似：一个经过训练的循环神经网络（RNN），用于接收语音频谱图并生成文本转录。

![](../../../figs.assets/image-20231122140018628.png)

<font color="red">图1：DS2系统的架构，用于对英语和普通话语音进行训练。我们通过改变卷积层的数量从1到3，以及循环或GRU层的数量从1到7来探索这个架构的变体。</font>

设从训练集$X = \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots \}$中采样得到单个话语$x^{(i)}$和标签$y^{(i)}$。每个话语$x^{(i)}$是一个长度为$T^{(i)}$的时间序列，其中每个时间切片是一个音频特征的向量，$x^{(i)}_t, t = 0, \ldots, T^{(i)}-1$。我们使用功率归一化的音频片段的频谱图作为系统的特征，因此$x^{(i)}_{t,p}$表示时间$t$处音频帧中第$p$个频率谱的功率。RNN的目标是将输入序列$x^{(i)}$转换为最终的转录$y^{(i)}$。为了符号简便，**我们省略上标，用$x$表示所选话语，$y$表示相应的标签**。

网络的输出是每种语言的字形。在每个输出时间步$t$，RNN对字符$p(l_t|x)$进行预测，其中 $l_t$ 可以是字母表中的字符，也可以是空白符号。在英语中，我们有$l_t \in \{a, b, c, \ldots, z, \text{space}, \text{apostrophe}, \text{blank}\}$，其中我们添加了撇号（apostrophe）以及空格符号表示单词边界。对于普通话系统，网络输出简体中文字符。我们将在第3.9节中对此进行更详细的描述。

RNN模型由多层隐藏单元组成。我们实验的架构包括一个或多个卷积层，接着是一个或多个循环层，最后是一个或多个全连接层。

在第$l$层的隐藏表示由$h^{(l)}$给出，按照惯例，$h^{(0)}$表示输入$x$。网络底部是对输入的时间维度进行一次或多次卷积。对于大小为$c$的上下文窗口，卷积层在时间步$t$的第$i$个激活由以下公式给出：


$$
 h^{(l)}_{t,i} = f(w^{(l)}_i \circledast h^{(l-1)}_{t-c:t+c}) 
$$


其中，◦表示第i个滤波器与前一层激活的上下文窗口之间的逐元素乘积，f表示一元非线性函数。我们使用修剪后的修正线性（ReLU）函数$\sigma(x) = \min(\max(0, x), 20)$ 作为我们的非线性激活函数。在某些层，通常是第一层，我们通过将卷积的步幅设为$s$帧来进行子采样。这样做的目的是缩短上面的循环层的时间步数。

在卷积层之后是一个或多个双向循环层[57]。正向时的激活$-\hat{h}^{(l)}$和反向时的激活$-\tilde{h}^{(l)}$分别计算如下：


$$
\hat{h}^{(l)}_t = g(h^{(l-1)}_t, -\hat{h}^{(l)}_{t-1}) \\
-h^{(l)}_t = g(h^{(l-1)}_t, -h^{(l)}_{t+1})
$$


这两组激活被求和以形成该层的输出激活$h^{(l)} = -\hat{h}^{(l)} + \tilde{h}^{(l)}$。函数$g(\cdot)$可以是标准的循环操作。


$$
\hat{h}^{(l)}_t = f(W_l \hat{h}^{(l-1)}_t + \tilde{U}_l \hat{h}^{(l)}_{t-1} + b_l)
$$


其中，$W_l$是输入到隐藏层的权重矩阵，$\hat{U}_l$是循环权重矩阵，$b_l$是偏置项。在这种情况下，输入到隐藏层的权重在循环的两个方向上是共享的。函数$g(\cdot)$还可以表示更复杂的循环操作，例如长短时记忆（LSTM）单元[30]和门控循环单元（GRU）[11]。

在双向循环层之后，我们应用一个或多个全连接层，其公式为：


$$
h^{(l)}_t = f(W_l h^{(l-1)}_t + b_l)
$$


输出层$L$是一个softmax层，计算给定如下概率分布的字符：


$$
p(l_t = k | x) = \frac{\exp(w^{(L)}_k \cdot h^{(L-1)}_t)}{\sum_j \exp(w^{(L)}_j \cdot h^{(L-1)}_t)}
$$


该模型使用CTC（Connectionist Temporal Classification）损失函数进行训练[22]。**对于给定的输入-输出对$(x, y)$和当前网络参数$\theta$，我们计算损失函数$L(x, y; \theta)$及其相对于网络参数的导数$\Delta_{\theta}L(x, y; \theta)$。**然后使用这个导数通过时间的反向传播算法来更新网络参数。

在接下来的小节中，我们描述了相对于DS1 [26]所做的体系结构和算法改进。除非另有说明，这些改进都是与语言无关的。我们报告了在一个英语说话者保留的开发集上的结果，这是一个包含2048个主要是朗读语音的内部数据集。所有模型均在第5节中描述的数据集上进行训练。我们报告了英语系统的词错误率（WER）和普通话系统的字符错误率（CER）。在两种情况下，我们在波束搜索解码步骤中集成了语言模型，如第3.8节所述。

### 深度 RNN 批归一化

为了在扩展训练集的同时有效地扩展我们的模型，我们通过增加更多隐藏层的方式来增加网络的深度，而不是使每一层更大。先前的工作已经通过增加连续的双向循环层的数量来研究这样做[24]。**我们探讨了批归一化（Batch Normalization，BatchNorm）作为一种加速这些网络训练的技术**[63]，因为它们通常遭受优化问题。

最近的研究表明，批归一化提高了循环神经网络的收敛速度，但并没有显示出在泛化性能上的改善[37]。相反，我们展示了当应用于大型数据集上的非常深的简单RNN网络时，批归一化显著提高了最终的泛化误差，同时大大加速了训练。

在典型的前馈层中，包含一个仿射变换，然后是一个非线性激活函数$f(\cdot)$，我们通过应用$f(B(W h))$而不是$f(W h + b)$来插入BatchNorm变换，其中：


$$
B(x) = \gamma \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} + \beta
$$


术语 $E$ 和 $Var$ 分别表示小批量上的经验均值和方差。由于均值移除已经抵消了其影响，层的偏置 $b$ 被省略。可学习的参数 $\gamma$ 和 $\beta$ 允许层按需缩放和移位每个隐藏单元。常数 $\epsilon$ 是一个小的正数，仅用于数值稳定性。在我们的卷积层中，对于给定的卷积滤波器，均值和方差是在小批量上估算的，考虑到了给定卷积层输出单元的所有时间。BatchNorm 变换通过隔离给定层的输入均值和方差的可能不相关的变化，减少了内部协方差漂移。

我们考虑将 BatchNorm 扩展到双向循环神经网络的两种方法[37]。一个自然的扩展是在每个非线性激活之前立即插入 BatchNorm 变换。公式3随之变为：


$$
\hat{h}^{(l)}_t = f\left(B(W_l \hat{h}^{(l-1)}_t + \tilde{U}_l \hat{h}^{(l)}_{t-1})\right)
$$


在这种情况下，均值和方差的统计是在小批量的单个时间步上累积的。时间步之间的时序依赖性阻止了对所有时间步的平均。我们发现，这种技术并没有改善优化。我们还尝试在连续的时间步上累积平均值，因此后面的时间步被规范化为所有当前和先前的时间步。这也证明是无效的，并且大大复杂化了反向传播。

我们发现，**序列级别的归一化**[37]克服了这些问题。循环计算如下：


$$
\hat{h}^{(l)}_t = f\left(B(W_l \hat{h}^{(l-1)}_t) + \tilde{U}_l \hat{h}^{(l)}_{t-1}\right)
$$


对于每个隐藏单元，我们计算整个小批量内所有项目在序列长度上的均值和方差统计。图2显示，**使用序列级别的归一化，深度网络的收敛速度更快**。表1显示，序列级别的归一化带来的性能提升随网络深度增加而增加，对于最深的网络，性能差异达到了12%。当比较深度时，为了控制模型大小，我们保持总参数数量不变，仍然看到了强大的性能增益。如果我们保持每层激活的数量不变并增加层数，我们预计深度将带来更大的改善。我们还发现，对于最浅的网络，BatchNorm 对泛化误差产生负面影响，就像对于较浅的网络它收敛较慢一样。

![](../../../figs.assets/image-20231122144919850.png)

<font color="red">图2：使用和不使用 BatchNorm 训练的两个模型的训练曲线。我们在第一个训练周期之后开始绘制图表，因为由于第3.3节中提到的 SortaGrad 课程方法，曲线更难解释。</font>

![](../../../figs.assets/image-20231122145401065.png)

<font color="red">表1：在不同深度的RNN上，使用和不使用BatchNorm的训练集和开发集的词错误率（WER）比较。随着深度的增加，参数数量保持不变，因此每层的隐藏单元数量减少。所有网络都有3800万参数。架构“M RNN，N total”表示在输入处有1层1D卷积，M个连续的双向RNN层，其余层为具有N个总层数的全连接层的网络。</font>



BatchNorm 方法在训练中效果很好，但在部署的自动语音识别（ASR）系统中实现起来很困难，因为在部署中通常需要评估单个语音而不是一个批次。我们发现将每个神经元归一化到其仅在序列上的均值和方差会降低性能。相反，我们在训练期间收集神经元的均值和方差的运行平均值，并在部署中使用这些值进行评估[63]。使用这种技术，我们可以逐个评估单个语音，比使用大批次进行评估的效果更好。

### SortaGrad

**在具有不同长度示例的训练中存在一些算法挑战**。一种可能的解决方案是通过时间截断反向传播[68]，以便在训练过程中所有示例具有相同的序列长度[52]。然而，这可能会抑制学习更长期依赖的能力。其他研究发现按照难度顺序呈现示例可以加速在线学习[6, 70]。许多序列学习问题（包括机器翻译和语音识别）中的一个共同主题是**更长的示例往往更具挑战性**[11]。

我们使用的CTC损失函数隐式地取决于话语的长度，


$$
L(x, y; \theta) = -\log \sum_{l \in \text{Align}(x, y)} \prod_{t=1}^{T} p_{ctc}(l_t | x; \theta)
$$


其中，$\text{Align}(x, y)$ 是在CTC运算符下将转录 $y$ 的字符对齐到输入 $x$ 的帧的所有可能对齐的集合。在上式中，内部项是序列时间步的乘积，随着序列长度的缩小而减小，因为$p_{\text{ctc}}(\text{'t} | x; \theta) < 1$。这启发了我们称之为SortaGrad的课程学习策略。SortaGrad以话语长度作为难度的启发式，因为长话语的成本高于短话语。

**在第一个训练时期，我们按照小批量中最长话语的长度递增的顺序迭代训练集**。在第一个时期之后，训练回到对小批量的随机顺序。表2显示了在具有7个循环层的9层模型上使用和不使用SortaGrad的训练成本的比较。这种效应在没有BatchNorm的网络中尤为明显，因为它们在数值上不太稳定。在某种程度上，这两种技术可以互相替代，尽管当应用SortaGrad和BatchNorm时我们仍然发现了收益。即使使用了BatchNorm，我们发现这个课程仍然提高了数值稳定性和对训练中小变化的敏感性。数值不稳定性可能是由于CPU和GPU中不同的超越函数实现，特别是在计算CTC成本时。这个课程为两种实现提供了可比较的结果。

![](../../../figs.assets/image-20231122150811299.png)

<font color="red">表2：在使用和不使用SortaGrad、以及使用和不使用批归一化的训练集和开发集上的词错误率（WER）比较。</font>

我们怀疑这些好处主要是因为长话语往往具有较大的梯度，然而**我们使用的学习率是独立于话语长度的固定值**。此外，长话语更有可能在训练的早期阶段导致RNN的内部状态爆炸。

### 简单 RNNs 和 GRUs 比较

到目前为止，我们展示的模型是简单的RNN，具有双向循环层，其前向和后向方向的递归由方程式3建模。当前语音和语言处理领域的研究表明，采用更复杂的递归结构可以使网络在更多的时间步上记住状态，但这也会增加训练的计算开销[52, 8, 62, 2]。**两种常用的递归结构是长短时记忆（LSTM）单元[30]和门控循环单元（GRU）[11]**，尽管还存在许多其他变种。最近对成千上万种LSTM和GRU架构变体的全面研究表明，具有适当初始化的遗忘门偏置的GRU与LSTM相当，并且它们的最佳变体相互竞争[32]。我们决定研究GRU，因为对较小的数据集进行的实验表明，对于相同数量的参数，GRU和LSTM达到了相似的准确性，但GRU的训练速度更快，而且不太可能发散。

GRU 通过如下进行计算


$$
z_t = \sigma(W_z x_t + U_z h_{t-1} + b_z) \\
r_t = \sigma(W_r x_t + U_r h_{t-1} + b_r) \\
\tilde{h}_t = f(W_h x_t + r_t \odot U_h h_{t-1} + b_h) \\
h_t = (1 - z_t)h_{t-1} + z_t \tilde{h}_t
$$


其中，$\sigma(\cdot)$ 表示 Sigmoid 函数，$z$ 和 $r$ 分别代表更新门和重置门，为简化起见，我们省略了层标记。与标准的 GRU 稍有不同的是，在通过重置门进行缩放之前，我们将隐藏状态 $h_{t-1}$ 乘以 $U_h$。这使得对 $h_{t-1}$ 的所有操作都可以通过单次矩阵乘法计算。输出非线性函数 $f(\cdot)$ 通常为双曲正切函数 $\tanh$。然而，我们发现对于 $\tanh$ 和 Clipped ReLU 非线性函数，性能相似，我们选择使用 Clipped ReLU，以保持简洁和与网络其余部分的一致性。

GRU 和简单 RNN 结构都受益于批标准化，并在深度网络中取得了强大的结果。然而，表3显示，**对于固定数量的参数，GRU 结构在所有网络深度下都实现了更好的词错误率（WER）**。这清晰地证明了语音识别任务中存在的长期依赖性，既存在于单个单词内部，也存在于单词之间。正如我们在第3.8节中讨论的那样，即使简单的 RNN 也能由于大量的训练数据而隐式地学习语言模型。有趣的是，具有5个或更多循环层的GRU网络并没有显著改善性能。我们将这归因于从每层1728个隐藏单元（对应1个循环层）稀疏到每层768个隐藏单元（对应7个循环层），以保持总参数数量不变。

![](../../../figs.assets/image-20231122152512825.png)

<font color="red">表3：对具有简单 RNN 或 GRU 的不同深度网络进行开发集词错误率（WER）的比较。所有模型均包含批标准化、一层1D不变卷积，并具有约3800万个参数。</font>

在表3中，GRU网络的性能优于简单RNN网络。然而，在后续的结果中（第6节），我们发现随着模型规模的扩大，在固定的计算预算下，简单RNN网络的性能略微更好。鉴于此，大多数剩余的实验中使用简单RNN层而不是GRU层。

### 频率卷积

