# 摘要总结

**2-Federated Learning Meets Natural Language Processing: A Survey**

论文链接：https://arxiv.org/abs/2107.12603v1

联邦学习旨在从多个分散的边缘设备（例如移动设备）或服务器上学习机器学习模型，而无需牺牲本地数据隐私。最近的自然语言处理技术依赖于深度学习和大型预训练语言模型。然而，大型深度神经网络和语言模型通常是通过大量数据在服务器端进行训练的。由于文本数据广泛来自最终用户，因此在这项工作中，我们研究了最近使用联邦学习作为学习框架的自然语言处理模型和技术。我们的调查讨论了联邦自然语言处理中的主要挑战，包括算法挑战、系统挑战以及隐私问题。我们还对现有的联邦自然语言处理评估方法和工具进行了批判性审查。最后，我们突出了当前的研究差距和未来方向。



**3-Federated Representation Learning for Automatic Speech Recognition**

论文链接：https://arxiv.org/abs/2308.02013

联邦学习（FL）是一种保护隐私的范例，允许边缘设备协作学习而无需共享数据。像Alexa和Siri这样的边缘设备是潜在的未标记音频数据源，可以用来学习强大的音频表征。在这项工作中，我们将自监督学习（SSL）和FL结合起来，以学习符合数据隐私约束的自动语音识别（ASR）表示。我们使用未标记的语音数据集Libri-Light中的说话者和章节信息来模拟非IID（非独立同分布）的说话者分隔数据分布，并使用FedSGD框架对LSTM编码器进行预训练。我们展示了在FL中预训练的ASR编码器表现与中央预训练模型一样好，并与无预训练相比，可以提高12-15%（WER）。我们进一步将联邦预训练模型调整到一种新的语言，法语，并展示相比无预训练的情况，可以提高20%（WER）。



**4-Training Speech Recognition Models with Federated Learning: A Quality/Cost Framework**

论文链接：https://arxiv.org/abs/2010.15965

我们提出使用联邦学习，一种分散的设备上学习范例，来训练语音识别模型。通过按用户进行每轮的训练，联邦学习必须承担处理非独立同分布（non-IID）数据分布的成本，这些数据分布预计会对训练模型的质量产生负面影响。我们提出了一个框架，通过这个框架，可以改变非独立同分布程度，从而展示模型质量与联邦训练的计算成本之间的权衡关系，我们通过一种新颖的度量来捕捉这种关系。最后，我们证明，通过超参数优化和适当使用变分噪声，可以弥补非独立同分布对模型质量的影响，同时减少成本。



**5-A federated approach in training acoustic models**

论文链接：http://www.interspeech2020.org/uploadfile/pdf/Mon-2-11-4.pdf



**6-End-to-End Speech Recognition from Federated Acoustic Models**

论文链接：https://ieeexplore.ieee.org/abstract/document/9747161



**7-Federated self-learning with weak supervision for speech recognition**

论文链接：https://ieeexplore.ieee.org/abstract/document/10096983