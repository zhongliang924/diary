# 摘要总结

**Federated Representation Learning for Automatic Speech Recognition**

论文链接：https://arxiv.org/abs/2308.02013

联邦学习（FL）是一种保护隐私的范例，允许边缘设备协作学习而无需共享数据。像Alexa和Siri这样的边缘设备是潜在的未标记音频数据源，可以用来学习强大的音频表征。在这项工作中，我们将自监督学习（SSL）和FL结合起来，以学习符合数据隐私约束的自动语音识别（ASR）表示。我们使用未标记的语音数据集Libri-Light中的说话者和章节信息来模拟非IID（非独立同分布）的说话者分隔数据分布，并使用FedSGD框架对LSTM编码器进行预训练。我们展示了在FL中预训练的ASR编码器表现与中央预训练模型一样好，并与无预训练相比，可以提高12-15%（WER）。我们进一步将联邦预训练模型调整到一种新的语言，法语，并展示相比无预训练的情况，可以提高20%（WER）。



**Training Speech Recognition Models with Federated Learning: A Quality/Cost Framework**

论文链接：https://arxiv.org/abs/2010.15965

我们提出使用联邦学习，一种分散的设备上学习范例，来训练语音识别模型。通过按用户进行每轮的训练，联邦学习必须承担处理非独立同分布（non-IID）数据分布的成本，这些数据分布预计会对训练模型的质量产生负面影响。我们提出了一个框架，通过这个框架，可以改变非独立同分布程度，从而展示模型质量与联邦训练的计算成本之间的权衡关系，我们通过一种新颖的度量来捕捉这种关系。最后，我们证明，通过超参数优化和适当使用变分噪声，可以弥补非独立同分布对模型质量的影响，同时减少成本。



**Federated Learning Meets Natural Language Processing: A Survey**

论文链接：https://arxiv.org/abs/2107.12603v1

联邦学习旨在从多个分散的边缘设备（例如移动设备）或服务器上学习机器学习模型，而无需牺牲本地数据隐私。最近的自然语言处理技术依赖于深度学习和大型预训练语言模型。然而，大型深度神经网络和语言模型通常是通过大量数据在服务器端进行训练的。由于文本数据广泛来自最终用户，因此在这项工作中，我们研究了最近使用联邦学习作为学习框架的自然语言处理模型和技术。我们的调查讨论了联邦自然语言处理中的主要挑战，包括算法挑战、系统挑战以及隐私问题。我们还对现有的联邦自然语言处理评估方法和工具进行了批判性审查。最后，我们突出了当前的研究差距和未来方向。



**Avoid Overfitting User Specific Information in Federated Keyword Spotting**

论文链接：https://arxiv.org/abs/2206.08864

关键词识别（KWS）旨在以精确高效的方式区分特定的唤醒词和其他信号，以满足不同用户的需求。最近的工作利用各种深度神经网络训练KWS模型，将所有用户的语音数据集中处理，而未考虑数据隐私问题。联邦关键词识别（FedKWS）可以作为一种解决方案，而无需直接共享用户的数据。然而，数据量较小、不同的用户习惯和不同的口音可能会导致严重问题，例如过拟合或权重发散。因此，我们提出了几种策略，以鼓励模型不过分拟合FedKWS中的用户特定信息。具体而言，我们首先提出了一种对抗性学习策略，该策略通过更新下载的全局模型来对抗过度拟合的本地模型，并明确鼓励全局模型捕获用户不变的信息。此外，我们提出了一种自适应本地训练策略，允许拥有更多训练数据和更均匀类别分布的客户端执行更多本地更新步骤。等效地，这种策略可以减弱那些数据质量较差的用户的负面影响。我们提出的FedKWS-UI可以在FedKWS中明确和隐含地学习用户不变的信息。丰富的实验结果在联邦Google语音命令上验证了FedKWS-UI的有效性。

