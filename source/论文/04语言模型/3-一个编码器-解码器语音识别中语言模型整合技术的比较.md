# 一个编码器-解码器语音识别中语言模型整合技术的比较

论文链接：https://arxiv.org/abs/1807.10857

基于注意力的循环神经编码器-解码器模型提供了一种优雅的解决方案，用于自动语音识别问题。**这种方法将声学模型、发音模型和语言模型融合成一个单一的网络，只需使用平行的语音和文本语料库进行训练**。然而，与传统方法将独立的声学和语言模型结合的情况不同，如何利用额外的（不配对的）文本并不明确。尽管之前有关解决这一问题的方法的研究，但对这些方法的深入比较仍然缺乏。在本文中，我们比较了一系列先前的方法和一些我们提出的方法，**用于利用不配对的文本数据来改进编码器-解码器模型**。为了评估，我们使用了中等规模的Switchboard数据集以及大规模的Google语音搜索和口述数据集。我们的结果证实了在多种方法和数据集上利用不配对的文本的好处。令人惊讶的是，在第一遍解码时，相对简单的浅层融合方法在各个数据集上表现最佳。然而，在Google数据集中，我们发现冷融合在Google语音搜索数据集上第二遍重评分后具有较低的 oracle 错误率，并在性能上优于其他方法。

**关键词**：语音识别；编解码器；语言模型；浅融合；冷融合；深融合

## 引言

基于注意力机制的循环神经编码器-解码器模型为语音识别、机器翻译和其他序列转导任务提供了一种优雅的端到端框架[1, 2]。在自动语音识别（ASR）中，该模型将传统上独立学习的声学模型、发音模型和语言模型（LM）合并为一个单一网络，可以进行端到端训练。编码器将输入语音映射到一系列高层次学到的特征，而解码器通过注意力机制辅助，将这些高层次特征映射到输出标签，提供了语音和文本之间的对齐。该模型可以进行端到端学习，只需要成对的语音和文本数据。最近，用于语音识别的编码器-解码器模型变得非常流行，并在许多ASR任务上表现竞争力[3–5]。

尽管端到端训练提供了几个优势，但也限制了训练数据必须具有输入和输出序列，例如在语音识别的情况下是成对的语音和文本数据。传**统的ASR模型利用在所有可用文本上训练的独立LM，其规模可以是音频转录文本的几个数量级**。编码器-解码器模型的解码器仅暴露于音频转录文本。

先前致力于解决利用不配对文本问题的研究提出了**将外部预训练的LM（在所有文本数据上训练）与ASR模型集成的方式**[6–8]。过去研究中主要的LM集成方法被称为浅层[6]、深层[6]和冷融合[7]。这三种方法在两个重要的标准上存在差异：

- 早期/晚期**模型集成**：在ASR模型的**计算过程**中，LM应该在什么时候集成？在深层和冷融合中，外部LM直接融入ASR模型，通过组合它们的隐藏状态，形成一个紧密集成的单一模型。相比之下，在浅层融合中，LM和ASR模型保持分离，只有它们的分数被组合，类似于一个集成模型。浅层融合的分数组合也类似于传统ASR中对声学模型和语言模型进行插值的方式。


- 早期/晚期**训练集成**：在ASR模型的**训练过程**中，LM应该在什么时候集成？深层和浅层融合使用晚期集成，其中ASR和LM模型分别进行训练，然后再合并。而冷融合则在ASR模型的训练一开始就使用外部预训练的LM模型。一个重要的观点是，如果两个模型中的任何一个经常变化，早期训练集成方法在计算上的成本较高。

据我们所知，目前对这些LM集成技术的深入比较尚不完备。在本文中，我们在（a）中等规模的Switchboard数据集[9]和（b）在[5]中使用的大规模Google语音搜索和口述数据集上比较了上述三种融合方法。我们的目标是揭示LM集成方法的比较情况，以及它们在数据规模上的扩展表现。我们还提出了一些新颖的LM集成方法，并将它们与Switchboard上的三种先前的融合方法进行比较。

我们的结果表明，几乎所有的LM集成方法都在所有数据集上相对于基线编码器-解码器模型表现出改进，证实了利用不配对文本的好处。我们还得出了一些其他发现：（a）在所有数据集上，相对简单的浅层融合方法在第一遍解码时表现最佳，（b）我们最佳的提出的方法在Switchboard数据集上的表现与深层和冷融合相似，（c）深层融合不具备良好的扩展性，在大规模Google数据集上相对于基线几乎没有增益，（d）冷融合产生高质量且多样的波束输出，在Google数据集上的 oracle 错误率最低，并在与Google语音搜索的第二遍LM重评分结合时领先。

## 相关工作

利用不配对文本进行编码器-解码器模型的先前研究可以分为两个主要主题：

**使用外部语言模型**

这种方法包括在不配对文本上训练外部语言模型，并将其整合到编码器-解码器模型中，这是本文的重点。Gulcehre等人[6]在这方面的早期研究提出了在神经机器翻译（NMT）模型背景下使用浅层和深层融合方法。在那项工作中，浅层和深层融合都改善了性能，尤其是对于资源较少的语言对，深层融合稍微优于浅层融合。Ramachandran等人[10]在NMT模型背景下的另一项先前研究提出了使用分别预训练的语言模型初始化编码器和解码器的较低层，然后通过同时使用语言建模和机器翻译损失进行联合训练。在ASR方面，浅层融合在一些研究中被广泛选择[3, 4, 8, 11]，取得了显著的性能提升，尽管在某些情况下对解码目标函数进行了轻微修改[4, 8, 11]。Sriram等人[7]为ASR提出了一种冷融合方法，这是对深层融合的修改。该研究发现，在约300-400K训练语音的中等规模数据集上，冷融合在跨域设置中表现优于深融合，但未与浅融合进行比较。这些研究中没有一项比较了这三种融合方法。

**从不配对文本生成配对数据**

第二个研究方向是利用不配对文本合成生成匹配的输入序列，从而扩展配对数据集。在机器翻译中，从单语数据生成配对数据的过程称为回译，即从不配对的目标语言文本生成源语言文本，已在神经机器翻译的背景下被Sennrich等人[12]使用。在ASR的情境中，直接类似的方法是使用文本到语音合成来从不配对文本生成语音。文本到语音（TTS）任务的复杂性意味着在从不配对文本生成的语音的使用方面已经进行了有限的研究，通常是在有限的设置中[13]。Renduchintala等人[14]尝试了一种解决方法，即将文本“翻译”为音素序列，并在多任务学习设置中使用生成的配对数据。

## 模型

我们的模型基于[1]提出的基于注意力的编码器-解码器语音识别（ASR）模型，即Listen, Attend and Spell (LAS) 模型。我们首先回顾该模型，然后描述我们考虑用于与LAS模型集成的LM的技术。

### LAS 模型

LAS模型由三个组件组成：编码器、解码器和注意力网络，它们一起进行训练以预测输出序列。转录可以被解码为一系列字母/字符、词片段或单词，这些来自一系列声学特征帧。基于基于词片段的模型在各种ASR任务和机器翻译任务中的最近成功经验[2, 3, 5, 15]，我们选择在所有我们的模型中使用词片段作为输出单元。

编码器由一堆叠的（双向的）循环神经网络（RNN）[16]​组成，它读取声学特征 $x = (x_1, \ldots, x_T)$ 并输出一系列高层次特征 $h$。高层次特征 $h$ 的序列可能与声学特征序列具有相同的长度，或者如果采用金字塔结构（如​[1]​中所述）则可能进行降采样。

解码器是一堆叠的单向循环神经网络（RNN），计算输出单元序列 $y$ 的概率，如下所示：


$$
P(y_j | x) = P(y_j | h) = \prod_{t=1}^{T} P(y_t | h; y_{<t})
$$


其中，$T$ 是输出序列的长度，$y_t$ 是输出序列的第 $t$ 个单元，$h$ 是高层次特征序列。

在每个时间步 $t$，通过注意力机制计算输出对编码器特征 $h$ 的条件依赖关系。注意力机制是当前解码器隐藏状态和编码器特征的函数，通过以下机制将编码器特征压缩成上下文向量 $c_t$：


$$
u_{it} = v^T \tanh(W_h h_i + W_d d_t + b_a) \\

\alpha_{it} = \text{softmax}(\mathbf{u}_t) \\

c_t = \sum_{i=1}^{T_x} \alpha_{it} h_i
$$


在上述公式中，向量 $v$、$b_a$ 以及矩阵 $W_h$、$W_d$ 是可学习的参数；$d_t$ 是解码器在时间步 $t$ 的隐藏状态。这些参数通过训练过程中学习得到，用于计算注意力权重和上下文向量。

解码器的隐藏状态 $d_t$，捕捉了先前的输出上下文 $y_{<t}$，由以下公式给出：


$$
d_t = \text{RNN}(\tilde{y}_{t-1}, d_{t-1}, c_{t-1})
$$


其中，$d_{t-1}$ 是解码器的前一个隐藏状态，$y_{t-1}$ 是 $y_{t-1}$ 的学习嵌入向量，这是基于RNN的语言模型中的典型做法。时间步 $t$ 处输出的后验分布由以下公式给出：


$$
P(y_t | h; y_{<t}) = \text{softmax}(W_s [c_t; d_t] + b_s)
$$


在上述公式中，$W_s$ 和 $b_s$ 再次是可学习的参数。该模型被训练以最小化判别损失：


$$
\mathcal{L}_{\text{LAS}} = -\log(P(y|x))
$$


其中，$P(y|x)$ 是模型生成输出序列 $y$ 给定输入序列 $x$ 的概率。这个损失函数用于衡量模型输出与真实标签之间的差异。

### LM 融合方法

下面我们讨论我们研究的编码器-解码器模型的各种语言模型（LM）集成方法。

#### 浅融合

在浅层融合中[6]，**外部语言模型仅在推理时通过对数线性插值进行整合**。因此，对于基线模型，使用束搜索来近似求解：


$$
 y^* = \arg \max_y \log P(y|x)
$$


在最基本版本的浅层融合中[6]，我们使用以下标准代替：


$$
y^* = \arg \max_y \log P(y|x) + \lambda \log P_{\text{LM}}(y)
$$


最近一些额外的惩罚项已经被引入到标准中[2, 4, 8, 11]。例如，Chorowski 和 Jaitly [4] 使用一个覆盖惩罚项 $c(x; y)$ 来确保在解码过程中所有的输入帧都被“充分关注”。

#### 深融合

与浅层融合类似，深层融合[6]是一种后期训练集成过程，即它假定编码器-解码器和语言模型已经预训练。其关键区别在于，**它通过以下方式将外部语言模型的隐藏状态（假设为神经语言模型）与解码器融合到一起**：


$$
g_t = \sigma(v^T_gd^{LM}_t + b_g) \\

d_{\text{DF}, t} = [c_t; d_t; g_t d^{LM}_t] \\

P(y_t | h; y_{<t}) = \text{softmax}(W_{\text{DF}} d^\text{DF}_t + b_{\text{DF}})
$$


其中，标量 $b_g$，向量 $v_g$ 和 $b_{\text{DF}}$，以及矩阵 $W_{\text{DF}}$ 都是可学习的参数，同时保持所有其他模型参数不变。固定大部分模型参数可以降低反向传播计算的成本，**与训练基线模型的成本相比，微调过程收敛较快**。

#### 冷融合

冷融合[7]建立在深层融合的基础上，提出了下面显示的修改语言模型（LM）集成过程：


$$
s_{\text{LM}, t} = \text{DNN}(d_{\text{LM}, t}) \\

s_{\text{ED}, t} = W_{\text{ED}} [d_t; c_t] + b_{\text{ED}} \\

g_t = \sigma(W_g[s_{\text{ED}, t}; s_{\text{LM}, t}] + b_g) \\

s_{\text{CF}, t} = [s_{\text{ED}, t}; g_t \odot s_{\text{LM}, t}] \\

r_{\text{CF}, t} = \text{DNN}(s_{\text{CF}, t}) \\

P(y_t | h; y_{<t}) = \text{softmax}(W_{\text{CF}} r_{\text{CF}, t} + b_{\text{CF}}) \\
$$


上述公式中引入的所有参数都是可学习的。冷融合与深融合之间的一些关键区别包括：

- 冷融合是一种早期训练的集成方法：编码器-解码器模型从头开始与预训练的外部语言模型 LM1 一起训练。


- 在门控计算中，既使用了语言模型状态 $s_{\text{LM}}$，也使用了编码器-解码器模型的状态 $s_{\text{ED}, t}$，如方程 8 所示。


- 与深融合使用的粗粒度门控机制（方程 4）相比，冷融合使用了细粒度的门控机制（方程 9）。


-  正如最初提出的，冷融合使用语言模型的 logits 而不是语言模型的隐藏状态，以实现灵活的语言模型交换。也就是说，在冷融合的提议版本中，方程 6 中的 $d_{\text{LM}, t}$ 是指语言模型的 logit 分数，而不是语言模型的隐藏状态。然而，在实践中，使用词片段作为输出单元，相对较大的词汇表导致了一个较长的 logit 向量 $d_{\text{LM}, t}$，这导致了参数数量的不必要增加。在我们的实验中，我们不关心交换语言模型的灵活性。因此，在我们的实验中，我们仍将 $d_{\text{LM}, t}$ 设置为语言模型的隐藏状态。

请注意，由于冷融合是一种早期训练的集成方法，在动态设置中，如果语言模型和自动语音识别模型经常变化，这种方法的计算成本将比前两种融合方法更高，特别是浅层融合。

#### 语言模型作为解码器底层

在机器翻译领域的先前工作中，有建议利用预训练的语言模型作为解码器底层的实用性[10]。类似地，[17]在语音识别的RNN转录模型中使用了预训练的语言模型来初始化解码器。采用这种方法的动机是，它可以提供更好的上下文化的词嵌入，就像最近提出的语言模型嵌入（ELMo）[18]一样。我们提出将外部语言模型作为预训练的LAS模型解码器的底层引入。包括语言模型参数在内的所有模型参数都使用LAS目标进行微调，微调进行了几个时期。

### 通过多任务学习的语言模型集成

回到解码器的方程1：



$$
P(y|x) = P(y|h) = \prod_{t=1}^{T_y} P(y_t|h; y_{<t}) + b_s
$$



可以看出解码器可以被看作是一个条件语言模型，以编码器特征为条件，这些特征表示语音输入。解码器对语音特征的确切依赖由上下文向量 $c_t$ 捕捉，从方程2中，它影响输出后验分布如下：



$$
P(y_t|h; y_{<t}) = \text{softmax}(W_s[c_t; d_t] + b_s)
$$



现在，不成对的文本没有对应的语音信号。在LAS模型中，这可以通过零上下文向量来表示。零上下文向量将解码器从条件语言模型降为纯语言模型，如下所示：



$$
P(y_t|\text{✓Sh}; y_{<t}) = \text{softmax}(W_s[\text{✚✚}_0c_t; d_t] + b_s)
$$



通过这种方式，解码器还可以用于语言建模任务。**基于这一观察，我们提出了一种使用不成对文本的多任务学习方法，其中LAS模型的解码器被用于主要的自动语音识别任务和辅助的语言模型任务**。在多任务学习的每次迭代中，我们根据选择每个任务的先验概率从ASR和LM任务中随机抽取一个任务。需要注意的一个重要方面是，与前面讨论的所有方法不同，**这种方法没有外部语言模型**；相反，**解码器本身被用于两个任务的训练**。

## 实验设置

### Switchboard

#### 数据

我们使用Switchboard语料库（LDC97S62）[9]，其中包含大约300小时的电话会话语音，作为我们中等规模训练集的选择。训练集的前4K个话语被保留为超参数调整和提前停止的验证集。由于训练集中有大量重复的短语音（yeah，uh-huh等），我们删除了超过300个计数阈值的重复项。经过这些预处理步骤，最终的训练集包含大约192K个语音。对于评估，我们使用HUB5 Eval2000数据集（LDC2002S09），其中包含两个子集：Switchboard（SWB），其风格与训练集相似，以及CallHome（CH），其中包含亲密朋友和家人之间的非脚本对话。对于声学特征，我们使用40维对数梅尔滤波器组特征以及它们的增量，进行每个说话者的均值和方差归一化。对于上述所有数据处理，我们使用EESEN工具包的配方[19]，该配方基于Kaldi工具包的配方[20]。

对于外部语言模型的训练，我们将Switchboard训练集与Fisher语料库（LDC200f4,5gT19）[21]结合在一起。为了避免领域不匹配，我们处理Fisher话语以（a）删除在Switchboard中未使用的噪声/犹豫标记，并（b）过滤掉在Switchboard上训练的字片段模型未涵盖的话语。过滤过程从2.2百万个Fisher话语中删除了大约400K个话语。因此，与Switchboard训练话语一起，LM在大约200万个话语上进行了训练。

#### 模型细节

编码器是一个4层金字塔形的双向长短时记忆（LSTM）网络[22]，导致时间分辨率降低8倍。对于在每个层次下面的顶层进行的2倍降低，我们对2个连续的隐藏状态进行最大池化，并将结果馈送到上一层。我们在每个方向的每一层中使用256个隐藏单元。

基线LAS模型中的解码器是一个单层的单向LSTM网络，具有256个隐藏单元。我们使用1K个字片段的输出词汇表，其中包括所有字符，以确保具有开放词汇的覆盖率。词汇表是使用byte pair encoding（BPE）算法的变体生成的[15]，该算法在Google的SentencePiece库中实现[6]。我们使用256维嵌入来表示与模型的其余部分一起学习的字片段。为了正则化，我们使用：（a）**标签平滑**[23]，其中我们在除地面实况标签之外的标签之间均匀分布0.1的概率质量，以及（b）**dropout**[24]，在所有RNN层的输出上应用概率0.1。我们还使用带有固定调度的计划采样[25]，其中每个时间步的解码器输入要么是地面实况先前标签的概率为0.9，要么是从先前标签的模型后验分布中采样的概率为0.1。

在推理阶段，我们使用beam search，其beam大小为10。我们观察到，对于一些模型，将beam大小增加到10会导致插入错误的增加，相比较小的beam大小。为了应对这一问题，我们添加了一个字片段插入奖励 $\{0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9\}$，在开发集上进行调优。通过添加字片段插入奖励，较大的beam大小在所有模型中均优于较小的beam大小，但在达到足够大的beam大小（10）后，增益微不足道。对于浅层融合，我们从 $\{0,0.5,0.1,0.15,0.2,0.25\}$ 选择LM权重$\lambda$，通过在开发集上进行调优，最终调整后的值为$\lambda=0.2$。

外部语言模型是一个具有512隐藏单元的单层RNN，其中包含LSTM单元。RNN隐藏状态首先通过具有256隐藏单元的投影层，最后馈送到softmax层。语言模型使用与LAS模型相同的输出词汇表进行训练。语言模型在20个时期内进行训练，根据开发集困惑度进行早停，并在Switchboard开发集上达到了约15的困惑度。

#### 训练细节

我们将我们的训练数据按话语长度分为5个桶，为了训练效率，我们限制一个小批量内的话语来自一个单独的桶。不同的桶使用不同的小批量大小，对于最短的话语，使用128的批量大小，对于最长的桶使用32的批量大小。初步实验表明，**在每个时期中，从具有最短话语到具有最长话语的桶依次遍历训练集会带来性能上的好处**。我们在训练所有模型时使用这个顺序（在[26]中也使用了类似的训练顺序方案）。所有模型均使用Adam优化器[27]进行训练，初始学习率为0.001。对于基线LAS模型和具有早期LM训练集成的模型，我们进行12个时期的训练，并在第7个时期后每个时期开始减半学习率。对于具有晚期LM训练集成的模型，我们在第4个时期后每个时期开始减半学习率，总共训练8个时期。

对于所有模型，我们在使用贪婪解码时使用基于开发集WER的早停。

为了加速训练，所有模型的编码器都使用与预测电话序列的LAS模型相同的编码器进行初始化，类似于[26]。所有模型都在一台NVIDIA TitanX GPU上进行训练，并在2天内完成训练，每个时期需要3-4个小时。最后，我们所有的模型都是用TensorFlow [28]实现的。

### 谷歌语音搜索和听写

#### 数据

训练数据包括约2200万条匿名的、由人类转录的话语，代表了Google实时流量，包括语音搜索和口述。清晰的话语被人工利用房间模拟器进行了损坏，添加了不同程度的噪音和混响，使得总体信噪比在0dB到30dB之间，平均信噪比为12dB。噪音来源包括来自YouTube和日常生活的嘈杂环境录音。模型在两个数据集上进行评估：VS14K，其中包含约14K条语音搜索话语，以及D15K，其中包含约15K条口述话语。

外部语言模型是在各种文本数据源上进行训练的，包括未转录的匿名语音查询（包括搜索和口述），来自Google搜索的匿名键入查询，以及上述提到的转录训练话语。由于这些组成数据源的大小不同，我们进行了上采样和下采样，使它们以1:1:1的比例混合。

#### 模型细节

我们的LAS模型与[5]一致：编码器由5个单向LSTM层组成，每个层有1400个隐藏单元，注意机制是一个具有四个头的多头加性注意力，解码器由2个单向LSTM层组成，每个层有1024个隐藏单元，输出词汇表是16384个字片段。我们使用80维的对数梅尔滤波器组特征，使用25ms窗口计算，并每10ms进行一次移动。类似于[29, 30]，在每个帧t，这些特征与左侧的3个帧叠加，并下采样到30ms的帧速率。

与[5]一样，推理是通过beam search进行的，使用8的beam大小。浅层融合的数值是在调整LM权重$\lambda$（在值$\{0,0.05,0.1,0.15,0.2,0.25,0.3,0.35\}$上）和覆盖惩罚（在值 $\{0,0.0.1,0.02,0.03,0.04,0.05\}$ 上）之后报告的，按照[8]的方法。这些参数在一个包含约10K语音搜索话语的开发集上进行了调优。

外部循环语言模型由2个2048隐藏单元的LSTM层组成。它具有与LAS模型相同的字片段输出词汇表。

#### 训练细节

LAS模型分为两个阶段进行训练。首先，它们使用交叉熵准则进行同步副本训练，直至收敛[5]。我们使用8x8的张量处理单元（TPUs）[31]，总共有128个同步副本和有效批量大小为4096。我们发现，拥有非常大的批量大小对于从冷融合中看到任何改善是至关重要的。我们的学习率调度包括一个初始热身阶段、一个恒定阶段和一个衰减阶段，与[5]一致。

接下来，我们进行第二个训练阶段，采用最小词错误率（MWER）准则[32]。这个阶段在16个同步GPU副本上进行，直至收敛，通常约为一个 epoch。请注意，对于深度融合，我们实际上有四个训练阶段：LAS的交叉熵训练、LAS的MWER训练、深度融合的交叉熵训练、深度融合的MWER训练。

外部语言模型也在拓扑结构为4x4的TPUs上进行训练。所有模型均使用Adam优化器[27]进行训练，并在TensorFlow[28]中实现。



